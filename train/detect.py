import sys
import os
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score

# å‡è®¾æ‚¨çš„è‡ªå®šä¹‰æ¨¡å—åœ¨é¡¹ç›®çš„æ ¹ç›®å½•ä¸‹
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# ç¡®ä¿å¯ä»¥æ‰¾åˆ°ä¹‹å‰çš„æ¨¡å—
from jepa.encoder import MyTimeSeriesEncoder
from patch_loader import get_loader

# =============================================================================
#  ç»„ä»¶å®šä¹‰: ä¸¥æ ¼ä½¿ç”¨ä¸Žæ‚¨è®­ç»ƒè„šæœ¬ä¸­ç›¸åŒçš„ç»„ä»¶
# =============================================================================

class StrongClassifier(nn.Module):
    """ç”¨äºŽä¸‹æ¸¸ä»»åŠ¡çš„åˆ†ç±»å™¨"""
    def __init__(self, input_dim, hidden_dim=256):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_dim // 2, 1)
        )
    def forward(self, x):
        return self.net(x)

def apply_instance_norm(x, eps=1e-5):
    """å¯¹è¾“å…¥ x çš„æ¯ä¸ªå®žä¾‹ç‹¬ç«‹è¿›è¡Œå½’ä¸€åŒ–ï¼Œä¸Žé¢„è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´"""
    mean = x.mean(dim=(2, 3), keepdim=True)
    std = x.std(dim=(2, 3), keepdim=True)
    return (x - mean) / (std + eps)

# ========= 1. å…¨å±€å’Œä¸‹æ¸¸ä»»åŠ¡è®¾ç½® =========
def setup_config():
    """é›†ä¸­ç®¡ç†æ‰€æœ‰é…ç½®"""
    config = {
        "DEVICE": torch.device("cuda" if torch.cuda.is_available() else "cpu"),
        "PRETRAINED_MODEL_PATH": "model/jepa_final.pt",
        "DATA_PATH_PREFIX": "data/TSB-AD-U/patches/", # è¯·æ ¹æ®æ‚¨çš„æ•°æ®é›†è·¯å¾„ä¿®æ”¹
        "DOWNSTREAM_EPOCHS": 50,
        "DOWNSTREAM_LR": 1e-3,
        "DOWNSTREAM_BATCH_SIZE": 512,
        "LATENT_DIM": 128,
        "X_PATCH_LENGTH": 30,
        "NUM_VARS": 1,
    }
    return config

# ========= 2. ä¸»æ‰§è¡Œå‡½æ•° =========
def main():
    """ä¸»æ‰§è¡Œé€»è¾‘"""
    cfg = setup_config()
    print("ðŸš€ [Step 1] Configuration loaded.")
    print(f"Using device: {cfg['DEVICE']}")

    # --- åŠ è½½æ•°æ®é›† ---
    print("\nðŸ“¦ [Step 2] Loading datasets for downstream task...")
    try:
        # ä½¿ç”¨éªŒè¯é›†è®­ç»ƒåˆ†ç±»å™¨
        train_classifier_loader = get_loader(
            npz_file=os.path.join(cfg["DATA_PATH_PREFIX"], "val.npz"),
            batch_size=cfg["DOWNSTREAM_BATCH_SIZE"],
            shuffle=True
        )
        # ä½¿ç”¨æµ‹è¯•é›†è¯„ä¼°åˆ†ç±»å™¨
        test_classifier_loader = get_loader(
            npz_file=os.path.join(cfg["DATA_PATH_PREFIX"], "test.npz"),
            batch_size=cfg["DOWNSTREAM_BATCH_SIZE"],
            shuffle=False
        )
        print("âœ… Datasets loaded successfully.")
    except FileNotFoundError as e:
        print(f"âŒ Error loading data: {e}. Please check your `DATA_PATH_PREFIX`.")
        return

    # --- åŠ è½½å¹¶å†»ç»“é¢„è®­ç»ƒEncoder ---
    print("\nðŸ§Š [Step 3] Loading and freezing pre-trained encoder...")
    pretrained_encoder = MyTimeSeriesEncoder(
        patch_length=cfg["X_PATCH_LENGTH"],
        num_vars=cfg["NUM_VARS"],
        latent_dim=cfg["LATENT_DIM"],
        time_layers=2, patch_layers=2, num_attention_heads=8,
        ffn_dim=cfg["LATENT_DIM"] * 4, dropout=0.4
    ).to(cfg["DEVICE"])

    try:
        state_dict = torch.load(cfg["PRETRAINED_MODEL_PATH"], map_location=cfg["DEVICE"])
        # åŠ è½½ online encoder çš„æƒé‡
        pretrained_encoder.load_state_dict(state_dict['encoder_online_state_dict'])
    except FileNotFoundError:
        print(f"âŒ Error: Pre-trained model not found at '{cfg['PRETRAINED_MODEL_PATH']}'.")
        return

    for param in pretrained_encoder.parameters():
        param.requires_grad = False
    pretrained_encoder.eval()
    print("âœ… Pre-trained encoder loaded and frozen.")

    # --- åˆå§‹åŒ–ä¸‹æ¸¸ä»»åŠ¡ç»„ä»¶ ---
    print("\nâœ¨ [Step 4] Initializing new classifier and optimizer...")
    downstream_classifier = StrongClassifier(input_dim=cfg["LATENT_DIM"]).to(cfg["DEVICE"])
    optimizer = torch.optim.AdamW(downstream_classifier.parameters(), lr=cfg["DOWNSTREAM_LR"])
    
    def compute_global_anomaly_ratio(loader):
        labels = [l for _, _, l in loader]
        return torch.cat(labels, dim=0).float().mean().item()

    anomaly_ratio = compute_global_anomaly_ratio(train_classifier_loader)
    pos_weight = torch.tensor([(1 - anomaly_ratio) / (anomaly_ratio + 1e-6)], device=cfg["DEVICE"])
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    print(f"Classifier initialized. Anomaly ratio in training data: {anomaly_ratio:.4f}")

    # --- è®­ç»ƒåˆ†ç±»å™¨ ---
    print("\nðŸ’ª [Step 5] Starting classifier training...")
    for epoch in range(1, cfg["DOWNSTREAM_EPOCHS"] + 1):
        downstream_classifier.train()
        total_loss = 0
        for x_batch, _, labels_batch in train_classifier_loader:
            x_batch, labels_batch = x_batch.to(cfg["DEVICE"]), labels_batch.to(cfg["DEVICE"])

            # =================================================================
            #  ä¸¥æ ¼ä»¿ç…§æ‚¨çš„è®­ç»ƒè„šæœ¬è¿›è¡Œæ•°æ®å¤„ç†å’Œç‰¹å¾æå–
            # =================================================================
            # 1. æ·»åŠ N_patchç»´åº¦ï¼Œä½¿è¾“å…¥å˜ä¸º4D: (B, 1, T, F)
            x_batch_4d = x_batch.unsqueeze(1)
            
            # 2. ä½¿ç”¨å®žä¾‹å½’ä¸€åŒ–
            x_batch_normed = apply_instance_norm(x_batch_4d)
            
            optimizer.zero_grad()
            
            # 3. ä½¿ç”¨å†»ç»“çš„encoderæå–ç‰¹å¾
            with torch.no_grad():
                features = pretrained_encoder(x_batch_normed) # Shape: [B, SEQ, D], æ­¤å¤„ SEQ=1

            # 4. ä¸¥æ ¼ä»¿ç…§L2/L3æŸå¤±çš„ç»´åº¦å¤„ç†æ–¹å¼
            B, SEQ, D = features.shape
            features_flat = features.reshape(B * SEQ, D)
            labels_expanded = labels_batch.view(-1, 1).repeat(1, SEQ).view(-1, 1).float()
            # =================================================================

            # 5. é€šè¿‡åˆ†ç±»å™¨å¾—åˆ°é¢„æµ‹
            logits = downstream_classifier(features_flat)
            
            # 6. è®¡ç®—æŸå¤±å¹¶æ›´æ–°åˆ†ç±»å™¨
            loss = criterion(logits, labels_expanded)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        avg_train_loss = total_loss / len(train_classifier_loader)
        print(f"[Epoch {epoch:02d}/{cfg['DOWNSTREAM_EPOCHS']}] Training... Train Loss: {avg_train_loss:.4f}")
    
    print("\nðŸ Training finished.")

    # --- åœ¨æœ€åŽä¸€ä¸ªepochåŽï¼Œè¿›è¡Œæœ€ç»ˆè¯„ä¼° ---
    print("\nðŸ§ª [Step 6] Starting final evaluation on the test set...")
    downstream_classifier.eval()
    all_preds, all_labels = [], []
    with torch.no_grad():
        for x_batch, _, labels_batch in test_classifier_loader:
            x_batch = x_batch.to(cfg["DEVICE"])

            # åŒæ ·åœ°ï¼Œä¸¥æ ¼ä»¿ç…§è®­ç»ƒè„šæœ¬è¿›è¡Œå¤„ç†
            x_batch_4d = x_batch.unsqueeze(1)
            x_batch_normed = apply_instance_norm(x_batch_4d)
            
            features = pretrained_encoder(x_batch_normed)
            
            B, SEQ, D = features.shape
            features_flat = features.reshape(B * SEQ, D)
            logits = downstream_classifier(features_flat)
            
            all_preds.append((logits > 0).cpu().int().numpy())
            all_labels.append(labels_batch.view(-1, 1).repeat(1, SEQ).view(-1, 1).numpy())

    all_preds = np.concatenate(all_preds)
    all_labels = np.concatenate(all_labels)
    
    f1 = f1_score(all_labels, all_preds, zero_division=0)
    acc = accuracy_score(all_labels, all_preds)
    prec = precision_score(all_labels, all_preds, zero_division=0)
    rec = recall_score(all_labels, all_preds, zero_division=0)

    print("\n--- âœ… Final Test Results ---")
    print(f"F1-Score:  {f1:.4f}")
    print(f"Accuracy:  {acc:.4f}")
    print(f"Precision: {prec:.4f}")
    print(f"Recall:    {rec:.4f}")
    print("--------------------------")

    print("\nðŸŽ‰ Downstream task evaluation finished!")

if __name__ == "__main__":
    main()