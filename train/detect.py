import os
import sys
import torch
import numpy as np
from torch.utils.data import DataLoader
from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix
from sklearn.ensemble import IsolationForest # æ ¸å¿ƒæ¨¡å‹

# --- åŠ å…¥é¡¹ç›®è·¯å¾„ ---
# ç¡®ä¿å¯ä»¥æ­£ç¡®å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# --- å¯¼å…¥è‡ªå®šä¹‰æ¨¡å— ---
from patch_loader import get_loader
from jepa.encoder import MyTimeSeriesEncoder

# ========== é…ç½® ==========
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_PATH = "model/jepa_best.pt"  # é¢„è®­ç»ƒçš„Encoderæ¨¡å‹è·¯å¾„
TRAIN_FEATURE_PATH = "data/MSL/patches/train.npz"
TEST_FEATURE_PATH = "data/MSL/patches/test.npz"
BATCH_SIZE = 256  # å¯ä»¥é€‚å½“è°ƒå¤§ä»¥åŠ å¿«ç‰¹å¾æå–é€Ÿåº¦
USE_INSTANCE_NORM = True  # æ§åˆ¶æ˜¯å¦ä½¿ç”¨æ—¶é—´ç»´åº¦å½’ä¸€åŒ–

def apply_time_norm(x, eps=1e-5):
    """
    å¯¹å•å˜é‡è¾“å…¥ (B, 1, T, 1) æ²¿æ—¶é—´ç»´åº¦è¿›è¡Œå½’ä¸€åŒ–
    è¾“å…¥:
        x: Tensor, shape (B, 1, T, 1) â€”â€” ä¸€ä¸ª batch çš„ patch åºåˆ—
    è¾“å‡º:
        normalized x, shape ç›¸åŒ
    """
    mean = x.mean(dim=2, keepdim=True)  # æ²¿æ—¶é—´ç»´åº¦
    std = x.std(dim=2, keepdim=True)
    return (x - mean) / (std + eps)

@torch.no_grad()
def extract_embeddings(encoder, data_loader, description):
    """ä½¿ç”¨encoderæå–æŒ‡å®šæ•°æ®é›†çš„æ½œåœ¨è¡¨ç¤º"""
    print(f"\næ­£åœ¨æå– {description} çš„ç‰¹å¾...")
    encoder.eval()
    
    all_embeddings = []
    all_labels = []

    for x_batch, _, labels_batch in data_loader:
        # unsqueeze(1) æ·»åŠ ä¸€ä¸ªç»´åº¦ä»¥åŒ¹é…æ¨¡å‹è¾“å…¥ (B, N_patch, T, F)
        x_batch = x_batch.to(DEVICE).unsqueeze(1)
        
        # === æ·»åŠ æ—¶é—´ç»´åº¦å½’ä¸€åŒ– ===
        if USE_INSTANCE_NORM:
            x_batch = apply_time_norm(x_batch)
        
        latent = encoder(x_batch)
        B, SEQ, D = latent.shape
        latent_flat = latent.reshape(B * SEQ, D)
        labels_flat = labels_batch.reshape(-1, 1)

        all_embeddings.append(latent_flat.cpu().numpy())
        all_labels.append(labels_flat.cpu().numpy())

    all_embeddings = np.concatenate(all_embeddings, axis=0)
    all_labels = np.concatenate(all_labels, axis=0)
    print(f"{description} ç‰¹å¾æå–å®Œæˆï¼Œå…± {len(all_embeddings)} ä¸ªæ ·æœ¬ã€‚")
    return all_embeddings, all_labels

def main():
    # ========== 1. åŠ è½½é¢„è®­ç»ƒçš„Encoder ==========
    print("ğŸš€ å¼€å§‹Encoder + Isolation Forestå¼‚å¸¸æ£€æµ‹æµç¨‹")
    print("\n[1/4] æ­£åœ¨åŠ è½½é¢„è®­ç»ƒçš„Encoderæ¨¡å‹...")
    
    try:
        checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)
    except FileNotFoundError:
        print(f"âŒé”™è¯¯: æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°äº '{MODEL_PATH}'ã€‚è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®ã€‚")
        return

    config = checkpoint.get("config")
    if config is None:
        print("âŒé”™è¯¯: æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸­ç¼ºå°‘ 'config' å­—å…¸ã€‚")
        return
        
    encoder_config = {
        'patch_length': config['patch_length'], 'num_vars': config['num_vars'], 'latent_dim': config['latent_dim'],
        'time_layers': 2, 'patch_layers': 2, 'num_attention_heads': 8, 'ffn_dim': config['latent_dim'] * 4, 'dropout': 0.3
    }

    encoder = MyTimeSeriesEncoder(**encoder_config).to(DEVICE)
    encoder.load_state_dict(checkpoint["encoder_online_state_dict"])
    print("âœ… EncoderåŠ è½½æˆåŠŸã€‚")

    # ========== 2. åŠ è½½æ•°æ®é›†å¹¶æå–ç‰¹å¾ ==========
    print("\n[2/4] æ­£åœ¨åŠ è½½æ•°æ®é›†å¹¶æå–ç‰¹å¾...")
    train_loader = get_loader(npz_file=TRAIN_FEATURE_PATH, batch_size=BATCH_SIZE, shuffle=False)
    test_loader = get_loader(npz_file=TEST_FEATURE_PATH, batch_size=BATCH_SIZE, shuffle=False)

    train_embeddings, train_labels = extract_embeddings(encoder, train_loader, "è®­ç»ƒé›†")
    test_embeddings, test_labels = extract_embeddings(encoder, test_loader, "æµ‹è¯•é›†")
    
    # ç­›é€‰å‡ºç”¨äºè®­ç»ƒçš„æ­£å¸¸æ ·æœ¬
    normal_train_mask = (train_labels == 0).flatten()
    normal_train_embeddings = train_embeddings[normal_train_mask]
    print(f"\nä»è®­ç»ƒé›†ä¸­ç­›é€‰å‡º {len(normal_train_embeddings)} ä¸ªæ­£å¸¸æ ·æœ¬ç”¨äºè®­ç»ƒIsolation Forestã€‚")
    
    # ========== 3. è®­ç»ƒIsolation Forestæ¨¡å‹ ==========
    print("\n[3/4] æ­£åœ¨è®­ç»ƒ Isolation Forest æ¨¡å‹...")
    # n_jobs=-1 ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„CPUæ ¸å¿ƒä»¥åŠ é€Ÿè®­ç»ƒ
    iso_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=42, n_jobs=-1)
    iso_forest.fit(normal_train_embeddings)
    print("âœ… Isolation Forest æ¨¡å‹è®­ç»ƒå®Œæˆã€‚")

    # ========== 4. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ ==========
    print("\n[4/4] æ­£åœ¨ä½¿ç”¨ Isolation Forest åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹ä¸è¯„ä¼°...")
    # æ¨¡å‹é¢„æµ‹: +1 ä»£è¡¨æ­£å¸¸ (inlier), -1 ä»£è¡¨å¼‚å¸¸ (outlier)
    test_preds_iso = iso_forest.predict(test_embeddings)
    
    # å°†é¢„æµ‹ç»“æœæ˜ å°„åˆ° 0/1 æ ‡ç­¾ (0: æ­£å¸¸, 1: å¼‚å¸¸)
    # è¿™æ˜¯è¯„ä¼°æŒ‡æ ‡æ‰€æœŸæœ›çš„æ ¼å¼
    test_preds_mapped = np.array([0 if p == 1 else 1 for p in test_preds_iso])
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    f1 = f1_score(test_labels, test_preds_mapped)
    precision = precision_score(test_labels, test_preds_mapped)
    recall = recall_score(test_labels, test_preds_mapped)
    cm = confusion_matrix(test_labels, test_preds_mapped)
    
    print("\n--- Isolation Forest åœ¨æµ‹è¯•é›†ä¸Šçš„æœ€ç»ˆè¯„ä¼°ç»“æœ ---")
    print(f"  - F1 Score:  {f1:.4f}")
    print(f"  - Precision: {precision:.4f}")
    print(f"  - Recall:    {recall:.4f}")
    print("  - Confusion Matrix:")
    print(f"    {cm}")
    print("-------------------------------------------------")
    print("\nğŸ‰ æµç¨‹ç»“æŸã€‚")

if __name__ == "__main__":
    main()