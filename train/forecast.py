import sys
import os
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from jepa.encoder import MyTimeSeriesEncoder
from jepa.predictor import JEPPredictor

# ========= è®¾ç½® =========
BATCH_SIZE = 4
LATENT_DIM = 64
EPOCHS     = 50
LEARNING_RATE = 1e-3
EARLY_STOPPING_PATIENCE = 10
EARLY_STOPPING_DELTA    = 1e-4

PATCH_FILE = "data/SOLAR/patches/solar_train.npz"
VAL_FILE   = "data/SOLAR/patches/solar_val.npz"
TEST_FILE  = "data/SOLAR/patches/solar_test.npz"

# æ¯ä¸ª patch çš„æ—¶é—´æ­¥é•¿å’Œå˜é‡æ•°
PATCH_LENGTH = 30
NUM_VARS     = 137
ONE_PATCH_SIZE = PATCH_LENGTH * NUM_VARS  # 4110

# ========= å·¥å…·å‡½æ•° =========
def prepare_batched_tensor(np_array: np.ndarray, batch_size: int) -> torch.Tensor:
    """
    å°†å½¢çŠ¶ (total_patches, T, F) çš„ numpy æ•°ç»„é‡ç»„ä¸º (B, N, T, F) å½¢å¼ï¼Œ
    å…¶ä¸­ B=batch_size, N=total_patches//B.
    """
    total, T, F = np_array.shape
    N = total // batch_size
    usable = batch_size * N
    return torch.tensor(
        np_array[:usable], dtype=torch.float
    ).view(batch_size, N, T, F)

def compute_metrics(pred: torch.Tensor, target: torch.Tensor) -> dict:
    """è®¡ç®—æ•´ä½“ MSE å’Œ MAE"""
    mse = nn.MSELoss()(pred, target)
    mae = nn.L1Loss()(pred, target)
    return {'mse': mse.item(), 'mae': mae.item()}

# ========= Step 1: åŠ è½½å¹¶å†»ç»“ encoder + predictor_short/predictor_long =========
print("\n[Step 1] Loading pretrained JEPA models...")

ckpt = torch.load('model/jepa_models.pt', map_location='cpu')
encoder = MyTimeSeriesEncoder(**ckpt['encoder_config'])
predictor_short = JEPPredictor(**ckpt['predictor_short_config'])
predictor_long  = JEPPredictor(**ckpt['predictor_long_config'])

encoder.load_state_dict(ckpt['encoder_state_dict'])
predictor_short.load_state_dict(ckpt['predictor_short_state_dict'])
predictor_long.load_state_dict( ckpt['predictor_long_state_dict'])

# å†»ç»“å‚æ•°
encoder.eval()
predictor_short.eval()
predictor_long.eval()
for p in encoder.parameters():        p.requires_grad = False
for p in predictor_short.parameters(): p.requires_grad = False
for p in predictor_long.parameters():  p.requires_grad = False

print("â†’ Encoder and both predictors are loaded and frozen.")

# ========= Step 2: è¯»å–å¹¶å‡†å¤‡è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ® =========
print("\n[Step 2] Loading patch data...")

# è®­ç»ƒé›†
npz_train = np.load(PATCH_FILE)
ctx_long_train  = prepare_batched_tensor(npz_train['long_term_context'],  BATCH_SIZE)  # [B, N_ctx, 30,137]
fut_long_train  = prepare_batched_tensor(npz_train['long_term_future'],   BATCH_SIZE)  # [B, N_fut, 30,137]

# éªŒè¯é›†
npz_val = np.load(VAL_FILE)
ctx_long_val  = prepare_batched_tensor(npz_val['long_term_context'],  BATCH_SIZE)
fut_long_val  = prepare_batched_tensor(npz_val['long_term_future'],   BATCH_SIZE)

# æµ‹è¯•é›†
npz_test = np.load(TEST_FILE)
ctx_long_test  = prepare_batched_tensor(npz_test['long_term_context'],  BATCH_SIZE)
fut_long_test  = prepare_batched_tensor(npz_test['long_term_future'],   BATCH_SIZE)

print(f"ctx_long_train:  {tuple(ctx_long_train.shape)}, fut_long_train:  {tuple(fut_long_train.shape)}")
print(f"ctx_long_val:    {tuple(ctx_long_val.shape)},   fut_long_val:    {tuple(fut_long_val.shape)}")
print(f"ctx_long_test:   {tuple(ctx_long_test.shape)},  fut_long_test:   {tuple(fut_long_test.shape)}")

# ========= Step 3: é¢„å…ˆè®¡ç®— latent-space é¢„æµ‹ï¼ˆä»…ä¸€æ¬¡ï¼‰ =========
print("\n[Step 3] Precomputing latent predictions for training/validation/test ...")

with torch.no_grad():
    # 3.a) æŠŠ context é€è¿› encoder å¾—åˆ° z_ctxï¼ˆ[B, N_ctx, latent_dim]ï¼‰
    z_ctx_train  = encoder(ctx_long_train)  # ä½¿ç”¨ long-term context
    z_ctx_val    = encoder(ctx_long_val)
    z_ctx_test   = encoder(ctx_long_test)

    # 3.b) æŠŠ fut patches ä¹Ÿè½¬ä¸º latent
    z_fut_train  = encoder(fut_long_train)  # [B, N_fut, latent_dim]
    z_fut_val    = encoder(fut_long_val)
    z_fut_test   = encoder(fut_long_test)

    # 3.c) predictor_long ç›´æ¥é¢„æµ‹ "æ‰€æœ‰ future latent"ï¼š
    z_pred_train_full, _ = predictor_long(z_ctx_train, z_fut_train)
    z_pred_val_full,   _ = predictor_long(z_ctx_val,   z_fut_val)
    z_pred_test_full,  _ = predictor_long(z_ctx_test,  z_fut_test)

# z_pred_*_full çš„å½¢çŠ¶å‡ä¸º [B, N_fut, latent_dim]


# ========= Step 4: å®šä¹‰ ForecastHeadï¼Œç”¨äºæŠŠ latentï¼ˆdim=64ï¼‰æ˜ å°„å›æ—¶åº patchï¼ˆdim=4110ï¼‰ =========
class ForecastHead(nn.Module):
    def __init__(self, latent_dim, output_dim):
        super().__init__()
        self.head = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, output_dim)
        )
    def forward(self, x):
        return self.head(x)


# åˆå§‹åŒ– ForecastHead
forecast_head = ForecastHead(
    latent_dim=LATENT_DIM,
    output_dim=ONE_PATCH_SIZE  # 30*137=4110
)
optimizer = torch.optim.Adam(forecast_head.parameters(), lr=LEARNING_RATE)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=5, verbose=True
)
loss_fn = nn.MSELoss()


# ========= Step 5: å°† "[B, N_fut, latent_dim]" å±•å¹³æˆ "[B*N_fut, latent_dim]"ï¼Œä»¥åŠå¯¹åº”ç›®æ ‡ "[B*N_fut, 4110]" =========
def flatten_for_head(z_pred_full, fut_patches):
    """
    è¾“å…¥ï¼š
      - z_pred_full: [B, N_fut, latent_dim]
      - fut_patches: [B, N_fut, T, F]
    è¿”å›ï¼š
      - z_flat:  [B*N_fut, latent_dim]
      - y_flat:  [B*N_fut, T*F]
    """
    B, N_fut, D = z_pred_full.shape
    # flatten latent
    z_flat = z_pred_full.view(B * N_fut, D)  # [B*N_fut, latent_dim]
    # flatten çœŸå®å€¼
    y_flat = fut_patches.view(B * N_fut, PATCH_LENGTH * NUM_VARS)  # [B*N_fut, 4110]
    return z_flat, y_flat

# è®­ç»ƒé›†ã€éªŒè¯é›†çš„å¹³å¦åŒ–
z_train_flat,  y_train_flat  = flatten_for_head(z_pred_train_full, fut_long_train)
z_val_flat,    y_val_flat    = flatten_for_head(z_pred_val_full,   fut_long_val)
# æµ‹è¯•é›†çš„å¹³å¦åŒ–
z_test_flat,   y_test_flat   = flatten_for_head(z_pred_test_full,  fut_long_test)

print(f"\nâ†’ z_train_flat: {tuple(z_train_flat.shape)},  y_train_flat: {tuple(y_train_flat.shape)}")
print(f"â†’ z_val_flat:   {tuple(z_val_flat.shape)},    y_val_flat:   {tuple(y_val_flat.shape)}")
print(f"â†’ z_test_flat:  {tuple(z_test_flat.shape)},   y_test_flat:   {tuple(y_test_flat.shape)}")

# ========= Step 6: åˆ’åˆ† ForecastHead çš„è®­ç»ƒ/éªŒè¯ splitï¼ˆå¦‚ 75% è®­ç»ƒï¼Œ25% éªŒè¯ï¼‰ =========
num_train = z_train_flat.size(0)
# æŒ‰ 75%/25% éšæœºåˆ’åˆ†
idx = torch.randperm(num_train)
n_tr = int(num_train * 0.75)
train_idx = idx[:n_tr]
val_idx   = idx[n_tr:]

z_head_tr, y_head_tr = z_train_flat[train_idx], y_train_flat[train_idx]
z_head_va, y_head_va = z_train_flat[val_idx],   y_train_flat[val_idx]

print(f"\n[Step 6] ForecastHead train/val split: {z_head_tr.shape[0]} è®­ç»ƒæ ·æœ¬, {z_head_va.shape[0]} éªŒè¯æ ·æœ¬")


# ========= Step 7: è®­ç»ƒ ForecastHead =========
print("\n[Step 7] Training ForecastHead ...")

train_losses = []
val_losses   = []
best_val_loss    = float('inf')
patience_counter = 0

for epoch in range(1, EPOCHS + 1):
    ## --- (1) è®­ç»ƒä¸€æ­¥ ---
    forecast_head.train()
    optimizer.zero_grad()
    pred_tr = forecast_head(z_head_tr)    # shape [n_tr, 4110]
    loss_tr = loss_fn(pred_tr, y_head_tr)  # å¯¹æ•´ä¸ª "B*N_fut" å±•å¹³åçš„ä¸€æ‰¹è®¡ç®— MSE
    loss_tr.backward()
    optimizer.step()
    train_losses.append(loss_tr.item())

    ## --- (2) éªŒè¯ä¸€æ­¥ ---
    forecast_head.eval()
    with torch.no_grad():
        pred_va = forecast_head(z_head_va)
        loss_va = loss_fn(pred_va, y_head_va)
        val_losses.append(loss_va.item())
        scheduler.step(loss_va)

        # Early stopping
        if loss_va < best_val_loss - EARLY_STOPPING_DELTA:
            best_val_loss = loss_va.item()
            torch.save(forecast_head.state_dict(), 'model/best_forecast_head.pt')
            patience_counter = 0
        else:
            patience_counter += 1

    print(f"[Epoch {epoch:02d}] Train MSE={loss_tr.item():.4f} | Val MSE={loss_va.item():.4f}")

    if patience_counter >= EARLY_STOPPING_PATIENCE:
        print(f"â†’ Early stopping at epoch {epoch}")
        break

# åŠ è½½æœ€ä¼˜æƒé‡
forecast_head.load_state_dict(torch.load('model/best_forecast_head.pt'))
print("[Step 7] Best ForecastHead loaded.")


# ========= Step 8: æµ‹è¯•æ—¶ç”¨ long-term predictor é¢„æµ‹ + ForecastHead å†æ˜ å°„å›æ—¶åºï¼Œè®¡ç®—æŒ‡æ ‡ =========
print("\n[Step 8] Evaluating on test set (long-term predictor) ...")
forecast_head.eval()
with torch.no_grad():
    # ä¸Šé¢å·²ç»é¢„è®¡ç®—å¥½äº† z_pred_test_full â†’ z_test_flat
    pred_test_flat = forecast_head(z_test_flat)  # [B*N_fut, 4110]
    # è®¡ç®— MSE/MAE
    metrics = compute_metrics(pred_test_flat, y_test_flat)
    print("Long-term predictor â†’ ForecastHead é¢„æµ‹ç»“æœï¼š")
    print(f"  MSE = {metrics['mse']:.4f}, MAE = {metrics['mae']:.4f}")


# ========= Step 9: å¯è§†åŒ– ForecastHead çš„è®­ç»ƒæ›²çº¿ =========
print("\n[Step 9] Plotting ForecastHead training history ...")
plt.figure(figsize=(10,6))
plt.plot(train_losses, label='Train MSE')
plt.plot(val_losses,   label='Val MSE')
plt.title("ForecastHead Training Curve")
plt.xlabel("Epoch")
plt.ylabel("MSE Loss")
plt.legend()
plt.grid(True)

plt.show()

# ========= å…¨éƒ¨å®Œæˆ =========
print("\nAll done! ğŸ˜Š")
